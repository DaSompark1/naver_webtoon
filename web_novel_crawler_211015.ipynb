{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T11:56:02.129057Z",
     "start_time": "2021-10-13T11:56:01.576384Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "import re\n",
    "import mechanicalsoup\n",
    "import pandas as pd\n",
    "import webbrowser\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import random\n",
    "import mechanicalsoup\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 네이버 시리즈 장르별 일간 top 100 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:45:20.639473Z",
     "start_time": "2021-10-13T14:29:36.458809Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "naver_novel = pd.DataFrame()\n",
    "label = ['title','genre','url','content','review_count']\n",
    "genre_num = ['201','202','207','208','206','203','205','209']\n",
    "for genre_id in genre_num:\n",
    "    for i in range(0,6):\n",
    "        url = f'https://series.naver.com/novel/top100List.series?rankingTypeCode=DAILY&categoryCode={genre_id}&page={i}'\n",
    "        try:\n",
    "            naver_headers = {}\n",
    "            print(url, i)\n",
    "            html = requests.get(url, headers = naver_headers, cookies = {}).text\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            product_count = len(soup.select('ul.comic_top_lst li'))\n",
    "            for p in range(0,product_count):\n",
    "                title = soup.select('ul.comic_top_lst li')[p].find_all('a')[1].get_text()\n",
    "                print(title)\n",
    "                detail_url = str(\"https://series.naver.com/\")+soup.select('ul.comic_top_lst li')[p].find('a')['href']\n",
    "                detail_html = requests.get(detail_url, headers=naver_headers, cookies={}).text\n",
    "                #x = random.uniform(3, 5)\n",
    "                #time.sleep(x)\n",
    "                print('세부페이지: ', detail_url)\n",
    "                body = BeautifulSoup(detail_html, 'html.parser')\n",
    "                #            body = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "                browser = mechanicalsoup.StatefulBrowser()\n",
    "                #더보기가 있으면 len = 2 없으면 len = 1\n",
    "                #re.sub('\\n','',url_soup.select('div._synopsis')[1].text)\n",
    "                try :\n",
    "                    a = body.select('div._synopsis')[1].text\n",
    "                except:\n",
    "                    a = body.select('div._synopsis')[0].text\n",
    "                #desc에 쓰여있는 정규식 제거하기\n",
    "                table = str.maketrans('\\t\\n\\r\\xa0\\s+','       ')\n",
    "                content = a.translate(table)\n",
    "                #장르\n",
    "                genre = body.select('li.info_lst li')[1].text\n",
    "                #리뷰 관련\n",
    "                bodyString = str(body)\n",
    "                jsonStr = '{'+bodyString[bodyString.find(\"ghtProductInfo\"):].split('}')[0].split('{')[1]+'}'\n",
    "                jsonStr = jsonStr.replace(\"'\",'\"')\n",
    "                dt = json.loads(jsonStr)\n",
    "                origin_id = int(dt['originalProductId'])\n",
    "                review_url = f'https://series.naver.com/comments/count.json?ticket=series5&objectId={origin_id}_0&lang=ko'\n",
    "                review_html = requests.get(review_url, headers = naver_headers, cookies = {}).text\n",
    "                review_count = json.loads(review_html, encoding=\"utf-8\")['result']['comment']\n",
    "                rx = [title,genre, detail_url, content,review_count]\n",
    "                naver_novel = naver_novel.append(pd.Series(rx, index = label), ignore_index = True)\n",
    "                x = random.uniform(0.5, 2.5)\n",
    "                time.sleep(x)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    #             if 'list index out of range' in str(e):\n",
    "    #                 continue\n",
    "    #             else:\n",
    "    #                 time.sleep(3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 네이버 웹툰 1570페이지까지 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-15T08:46:17.344217Z",
     "start_time": "2021-10-15T08:44:33.472016Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import requests\n",
    "import re\n",
    "import mechanicalsoup\n",
    "import pandas as pd\n",
    "import webbrowser\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import csv\n",
    "# title, 마지막 연재일, 세부 url 가져오기\n",
    "# page는 for문/ code = order\n",
    "# 200page까지 돌리기\n",
    "# label = ['title','genre','detail_url','date','review','content']\n",
    "# f = open('naver_novel_tot.csv', 'a+', encoding = 'utf-8', newline='') # make new file if file doesn't exist\n",
    "\n",
    "# wr = csv.writer(f)\n",
    "# wr.writerow([label[0], label[1], label[2], label[3], label[4], label[5]])\n",
    "f.close()\n",
    "naver_novel = pd.DataFrame()\n",
    "options = webdriver.ChromeOptions()\n",
    "#agent가 없으면 bot으로 생각함\n",
    "#options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 6.1; WOW64; Trident/7.0; rv:11.0) like Gecko\")\n",
    "#driver = webdriver.Chrome(executable_path = './chromedriver',options=options)\n",
    "#650~850\n",
    "for page in range(640,650):\n",
    "    print(\"페이지번호: \", page)\n",
    "    # 페이지별 화면 켜기\n",
    "    list_url = f'https://series.naver.com/novel/categoryProductList.series?categoryTypeCode=all&genreCode=&orderTypeCode=new&is&isFinished=false&page={page}'\n",
    "    naver_headers = {}\n",
    "    html = requests.get(list_url, headers=naver_headers, cookies={}).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    browser = mechanicalsoup.StatefulBrowser()\n",
    "    product_count = len(soup.select('ul.lst_list div'))\n",
    "    for i in range(0, product_count):\n",
    "        try:\n",
    "            rx = []\n",
    "            k = soup.select('ul.lst_list div')[i]\n",
    "            title = k.find('a')['title']\n",
    "            #print('제목: ', title)\n",
    "            #날짜는 첫번째에 들어온다!\n",
    "            tt = k.select('p.info')[0]\n",
    "            date = tt.text.replace('\\n','').split('|')[2]\n",
    "            detail_url = str(\"https://series.naver.com/\") + k.find('a')['href']\n",
    "            #세부 정보 url 열기\n",
    "    #             driver = webdriver.Chrome(\"./chromedriver\")\n",
    "    #             driver.get(detail_url)\n",
    "            #print(\"세부화면 켜기: \", detail_url)\n",
    "            detail_html = requests.get(detail_url,headers=naver_headers, cookies={}).text\n",
    "            body = BeautifulSoup(detail_html, 'html.parser')\n",
    "    #            body = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            browser = mechanicalsoup.StatefulBrowser()\n",
    "            #리뷰 관련\n",
    "            bodyString = str(body)\n",
    "            jsonStr = '{'+bodyString[bodyString.find(\"ghtProductInfo\"):].split('}')[0].split('{')[1]+'}'\n",
    "            jsonStr = jsonStr.replace(\"'\",'\"')\n",
    "            dt = json.loads(jsonStr)\n",
    "            origin_id = int(dt['originalProductId'])\n",
    "            review_url = f'https://series.naver.com/comments/count.json?ticket=series5&objectId={origin_id}_0&lang=ko'\n",
    "            review_html = requests.get(review_url, headers = naver_headers, cookies = {}).text\n",
    "            review = json.loads(review_html, encoding=\"utf-8\")['result']['comment']\n",
    "            #print(review)\n",
    "            #더보기가 있으면 len = 2 없으면 len = 1\n",
    "            #re.sub('\\n','',url_soup.select('div._synopsis')[1].text)\n",
    "            try :\n",
    "                a = body.select('div._synopsis')[1].text\n",
    "            except:\n",
    "                a = body.select('div._synopsis')[0].text\n",
    "            #desc에 쓰여있는 정규식 제거하기\n",
    "            table = str.maketrans('\\t\\n\\r\\xa0\\s+','       ')\n",
    "            content = a.translate(table)\n",
    "            #장르\n",
    "            genre = body.select('li.info_lst li')[1].text\n",
    "    #            driver.close()\n",
    "            rx = [title,genre, detail_url, date.replace('\\t',''), review, content]\n",
    "            naver_novel = naver_novel.append(pd.Series(rx, index = label), ignore_index = True)\n",
    "            f = open(f'naver_novel_tot.csv', 'a+', encoding = 'utf-8', newline='')\n",
    "            wr = csv.writer(f)\n",
    "            wr.writerow(rx)\n",
    "            f.close()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            if 'list index out of range' in str(e):\n",
    "                continue\n",
    "            else:\n",
    "                time.sleep(36)\n",
    "naver_novel = naver_novel[['title','genre','date','url','review','content']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 네이버시리즈 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T08:26:55.036170Z",
     "start_time": "2021-10-13T08:18:43.548310Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "naver_novel = pd.DataFrame()\n",
    "weekdays = ['MON','TUE','WED','THU','TRI','SAT','SUN']\n",
    "label = ['title','detail_url','download','content','genre','like']\n",
    "for week in weekdays:    \n",
    "    list_url = f'https://novel.naver.com/webnovel/weekdayList?week={week}&genre=all&order=Read'\n",
    "    naver_headers = {}\n",
    "    html = requests.get(list_url, headers=naver_headers, cookies={}).text\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    browser = mechanicalsoup.StatefulBrowser()\n",
    "    product_count = len(soup.select('ul.list_type1 li'))\n",
    "    print('작품개수: ',product_count)\n",
    "    for i in range(0,product_count):\n",
    "        rx = []\n",
    "        body = soup.select('ul.list_type1 li')[i]\n",
    "        title = body.find('a')['title']\n",
    "        print('제목: ', title)\n",
    "        detail_url = str(\"https://novel.naver.com\") + body.find('a')['href']\n",
    "        print(detail_url)\n",
    "        detail_html = requests.get(detail_url, headers=naver_headers, cookies={}).text\n",
    "        detail_soup = BeautifulSoup(detail_html, 'html.parser')\n",
    "        browser = mechanicalsoup.StatefulBrowser()\n",
    "        download = detail_soup.select('div.section_area_info span.download')[0].get_text()\n",
    "        content = detail_soup.select('div.section_area_info p.dsc')[0].get_text()\n",
    "        genre = detail_soup.select('div.section_area_info span.genre')[0].get_text()\n",
    "        like = detail_soup.select('div.section_area_info span.like')[0].get_text()\n",
    "        rx = [title,detail_url,download,content,genre,like]\n",
    "        naver_novel = naver_novel.append(pd.Series(rx, index = label), ignore_index = True)\n",
    "        x = random.uniform(0.5, 2.5)\n",
    "        time.sleep(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 카카오페이지 top 300 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T12:42:25.148422Z",
     "start_time": "2021-10-13T12:38:22.278848Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "label = ['title','read_count','date','genre','content','like_count','review']\n",
    "\n",
    "# 무한 스크롤\n",
    "# SCROLL_PAUSE_TIME = 2\n",
    "# last_height = driver.execute_script(\"return document.body.scrollHeight\")  \n",
    "# while True:\n",
    "#     driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "#     time.sleep(SCROLL_PAUSE_TIME)                                     \n",
    "#     driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight-50);\") \n",
    "#     time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "# # Calculate new scroll height and compare with last scroll height           \n",
    "#     new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "#     if new_height == last_height:                                               \n",
    "#         break\n",
    "\n",
    "#     last_height = new_height\n",
    "kakao_novel = pd.DataFrame()\n",
    "page = 1\n",
    "\n",
    "category = ['86','120','89','117','87']\n",
    "for category_num in category:\n",
    "    print(\"장르: \",category_num)\n",
    "    for page in range(0,12):\n",
    "        time.sleep(1.5)\n",
    "        print('페이지수:',page)\n",
    "        url = f'https://api2-page.kakao.com/api/v1/store/today_rank/list?page={page}&category=11&subcategory={category_num}'\n",
    "        driver = webdriver.Chrome(\"./chromedriver\")\n",
    "        driver.get(url)\n",
    "        body = BeautifulSoup(driver.page_source, 'html.parser').get_text()\n",
    "        body = json.loads(body, encoding=\"utf-8\")\n",
    "        driver.close()\n",
    "        product_count = len(body['list'])\n",
    "        for i in range(0, product_count):\n",
    "            rx = []\n",
    "            kakao_detail = body['list'][i]\n",
    "            title = kakao_detail['title']\n",
    "            print('제목: ',title)\n",
    "#            caption = kakao_detail['caption']\n",
    "            read_count = kakao_detail['read_count']\n",
    "            date = kakao_detail['last_slide_added_date']\n",
    "            genre = kakao_detail['sub_category_title']\n",
    "            #kakao_detail['publisher']\n",
    "            content = kakao_detail['description']\n",
    "            table = str.maketrans('\\t\\n\\r\\xa0\\s+','       ')\n",
    "            content = content.translate(table)\n",
    "            like_count = kakao_detail['like_count']\n",
    "            review = kakao_detail['comment_count']\n",
    "            #kakao_detail['like_count']\n",
    "            rx = [title, read_count, date, genre, content, like_count, review]\n",
    "            kakao_novel = kakao_novel.append(pd.Series(rx, index = label), ignore_index = True)\n",
    "    \n",
    "    \n",
    "kakao_novel = kakao_novel[['title','read_count','date','genre','content','like_count','review']]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:49:14.760805Z",
     "start_time": "2021-10-13T14:49:14.757536Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "naver_novel_fin = naver_novel[['title','genre','content','review_count','url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:49:15.005854Z",
     "start_time": "2021-10-13T14:49:15.001212Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "naver_novel_fin['platform'] = '<h1>네이버</h1>'\n",
    "naver_novel_fin = naver_novel_fin[['platform','title','genre','content','review_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:49:18.634962Z",
     "start_time": "2021-10-13T14:49:18.631288Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kakao_novel['platform'] = '<h1>카카오</h1>'\n",
    "kakao_novel_fin = kakao_novel[['platform','title','genre','content','review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:49:23.412274Z",
     "start_time": "2021-10-13T14:49:23.409194Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "naver_novel_fin.rename(columns = {\"review_count\": \"review\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:49:25.306812Z",
     "start_time": "2021-10-13T14:49:25.303378Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fin_novel_df = pd.concat([naver_novel_fin, kakao_novel_fin]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T14:49:40.026733Z",
     "start_time": "2021-10-13T14:49:39.709322Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fin_novel_df.to_excel('popular_novel.xlsx',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T13:35:22.310094Z",
     "start_time": "2021-10-13T13:35:22.306873Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "naver_novel_fin['genre'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-13T13:35:30.921983Z",
     "start_time": "2021-10-13T13:35:30.918043Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kakao_novel_fin['genre'].unique()\n",
    "#판타지 / 현판 / 로맨스 / 무협"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
